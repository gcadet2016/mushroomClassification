{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4fa3a8",
   "metadata": {},
   "source": [
    "# infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e26ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle basé sur les 10 espèces représentées\n",
    "\n",
    "\n",
    "# Vérifier que les chemins soient correct avant toutes opérations\n",
    "chemin_images = '../../images/'\n",
    "chemin_csv = '../data/top10.csv'\n",
    "\n",
    "# Définition du DataFrame echantillon (utile pour tests modèles car entrainements très rapides)\n",
    "pourcentage_echantillon = 0.1 # Si 0.1 : 10% du contenu\n",
    "\n",
    "# Dimensions des images\n",
    "img_dim = (224,224)\n",
    "img_shape = (224,224,3)\n",
    "\n",
    "# Taille des batchs\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b4ebe",
   "metadata": {},
   "source": [
    "##### A faire sur le notebook :\n",
    "\n",
    "- Terminer la programmation optuna\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d86c0",
   "metadata": {},
   "source": [
    "# Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802d727",
   "metadata": {},
   "source": [
    "Si le notebook tourne sur colab, charger les fichiers images (format zip) et dezipper en suivant les cellules qui suivent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b6d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les images en format .zip\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a54efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter le Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dezipper le fichier\n",
    "!unzip '/content/drive/MyDrive/SAS/images.zip' -d '/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147d4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les chemins en corrélation avec les dossiers colab\n",
    "chemin_images = '/images/images/'\n",
    "chemin_csv = '/content/drive/MyDrive/SAS/Jul23_bds_champignons/data/top10.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c948c0ec",
   "metadata": {},
   "source": [
    "# Librairies à charger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19d18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies utilisées par les fonctions\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "\n",
    "# Librairies utilisées pour les callbacks\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.keras.callbacks import TerminateOnNaN\n",
    "\n",
    "\n",
    "# Librairies utilisées pour créer les pipelines et le modèle\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Librairies utilisées pour optimise rle modèle*\n",
    "import optuna\n",
    "import optuna.visualization as optuna_viz\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# Librairies utilisées pour la création des jeux d'entrainement, de test et de validation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Librairies utilisées pour l'évaluation du modèle une fois entrainé\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f4ad3",
   "metadata": {},
   "source": [
    "# Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd70f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_df(chemin_images, chemin_csv, pourcentage_echantillon):\n",
    "    '''Importe le fichier csv et construit 2 df :\n",
    "        - Le DF basé sur le CSV original\n",
    "        - Un DF echantillon comportant 10% de données aléatoires du DF original\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # import du df\n",
    "    df = pd.read_csv(chemin_csv, low_memory=False)\n",
    "    df['image_url'] = df['image_url'].str.replace('.../images/', chemin_images)\n",
    "    print(f\"Nombre d'images chargées pour df: {df.shape[0]}\")\n",
    "    print(f\"Nb especes dans df: {df['label'].nunique()}\")\n",
    "\n",
    "\n",
    "    # Contruction de l'echantillon\n",
    "    L = len(df)\n",
    "    L_ech = int(pourcentage_echantillon * L)\n",
    "    df_ech = df.sample(n=L_ech, random_state=10)\n",
    "    df_ech.reset_index(inplace=True, drop=True)\n",
    "    print(f\"Nombre d'images chargées pour df_ech: {df_ech.shape[0]}\")\n",
    "    print(f\"Nb especes dans df_ech: {df_ech['label'].nunique()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return df, df_ech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d90200f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_img(image_path, label):\n",
    "\n",
    "    '''Modifie les images aléatoirement dans le dataset qui sera soumis au modèle, oversample les classes sous représentées.\n",
    "       image_path : chemin des images (variable définie en début de notebook),\n",
    "       label : Variable contenant les classes,\n",
    "   '''\n",
    "\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, img_dim)         # Rappel : img_dim est définie en début de Notebook\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = (img - tf.math.reduce_min(img)) / (tf.math.reduce_max(img) - tf.math.reduce_min(img))\n",
    "\n",
    "\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8d5d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(image_path, labels, batch_size, oversample_cls = None):\n",
    "    '''\n",
    "    Créé un dataset Tensorflow selon les paramètres précisés. La fonction oversample les classes sous représentées\n",
    "    image_path : chemin relatif de la variable contenant les images\n",
    "    labels : variable contenant les labels\n",
    "    batch_size : taille des batchs\n",
    "    oversample_cls : Liste contenant les classes à oversampler. Si non précisé, l'oversample sera ignoré\n",
    "    '''\n",
    "\n",
    "    image_path = image_path.tolist()  # Convertir les chemins d'images en liste\n",
    "    labels = labels.tolist()          # Convertir les labels en liste\n",
    "\n",
    "\n",
    " # Oversample des classes\n",
    "    if oversample_cls:\n",
    "    # Compter le nombre d'exemples par classe\n",
    "        class_counts = Counter(labels)\n",
    "\n",
    "    # Calculer le nombre d'exemples à ajouter pour chaque classe à oversampler\n",
    "        max_count = max(class_counts.values())\n",
    "        facteurs_oversample = {cls: max_count / count for cls, count in class_counts.items() if cls in oversample_cls}\n",
    "\n",
    "    # Répéter les exemples des classes à oversampler pour atteindre le nombre maximum\n",
    "        oversampled_image_paths = []\n",
    "        oversampled_labels = []\n",
    "        for img_path, label in zip(image_path, labels):\n",
    "            facteurs_oversample = facteurs_oversample.get(label, 1.0)\n",
    "            nb_copies = int(facteurs_oversample)\n",
    "            for _ in range(nb_copies):\n",
    "                oversampled_image_paths.append(img_path)\n",
    "                oversampled_labels.append(label)\n",
    "\n",
    "        image_path = oversampled_image_paths\n",
    "        labels = oversampled_labels\n",
    "\n",
    "\n",
    "\n",
    "    # Construction du Dataset    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_path, labels))\n",
    "    dataset = dataset.map(augment_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=len(image_path))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e07a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def controle_presence_fichiers(df, chemin_images):\n",
    "\n",
    "    ''' Controle que les fichiers images soient bien présents sur le disque.'''\n",
    "\n",
    "    image_directory = chemin_images\n",
    "    missing_files = []\n",
    "\n",
    "# Parcourir chaque ligne du DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        image_path = os.path.join(image_directory, row['image_lien'])\n",
    "    \n",
    "        if not os.path.exists(image_path):\n",
    "            missing_files.append(image_path)\n",
    "\n",
    "    # Afficher les fichiers non trouvés\n",
    "    if missing_files:\n",
    "        print(\"\\nFichiers non trouvés :\")\n",
    "        for file_path in missing_files:\n",
    "            print(file_path)\n",
    "    else:\n",
    "        print(\"\\nTous les fichiers sont présents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29075218",
   "metadata": {},
   "source": [
    "# Optuna - Fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1312715",
   "metadata": {},
   "source": [
    "La bibliothèque Optuna est une bibliothèque open source en Python qui est principalement utilisée pour l'optimisation des hyperparamètres, également connue sous le nom d'optimisation automatique des hyperparamètres (AutoML). Elle permet d'automatiser le processus de recherche des meilleures combinaisons d'hyperparamètres pour les modèles d'apprentissage automatique, ce qui peut grandement améliorer les performances des modèles.\n",
    "\n",
    "**Voici les principales utilisations et fonctionnalités de la bibliothèque Optuna :**\n",
    "- Optimisation des hyperparamètres : Optuna peut rechercher automatiquement les meilleures valeurs d'hyperparamètres pour un modèle donné en minimisant ou maximisant une fonction objectif. Les hyperparamètres sont des paramètres qui ne sont pas appris par le modèle lui-même, mais qui affectent ses performances, tels que le taux d'apprentissage, la profondeur du réseau de neurones, la taille du lot, etc.\n",
    "\n",
    "- Gestion des essais : Optuna gère la recherche des hyperparamètres en effectuant une recherche efficace dans l'espace des hyperparamètres en utilisant des algorithmes d'optimisation tels que l'optimisation des arbres de décision, l'optimisation bayésienne, etc. Il maintient un historique des essais antérieurs pour guider la recherche.\n",
    "\n",
    "- Intégration avec les frameworks de machine learning : Optuna peut être utilisé avec différents frameworks d'apprentissage automatique, tels que TensorFlow, PyTorch, Scikit-Learn, XGBoost, LightGBM, etc. Il est donc polyvalent et peut être utilisé pour optimiser divers types de modèles.\n",
    "\n",
    "- Extensible : Optuna est extensible, ce qui signifie que vous pouvez définir votre propre espace d'hyperparamètres à rechercher et définir des objectifs personnalisés en fonction de votre problème spécifique.\n",
    "\n",
    "- Parallélisme : Optuna prend en charge le parallélisme, ce qui signifie que vous pouvez effectuer plusieurs essais en parallèle pour accélérer le processus d'optimisation.\n",
    "\n",
    "- Visualisation des résultats : Optuna offre des outils de visualisation pour vous permettre d'analyser les résultats de l'optimisation, tels que les graphiques d'importance des hyperparamètres, les courbes d'apprentissage, etc.\n",
    "\n",
    "\n",
    "En résumé, Optuna est une bibliothèque puissante pour l'optimisation des hyperparamètres qui permet d'automatiser et de rationaliser le processus de recherche des meilleures configurations de modèle, ce qui peut vous faire gagner du temps et améliorer considérablement les performances de vos modèles d'apprentissage automatique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50b2192",
   "metadata": {},
   "source": [
    "### Modèle pré-entrainé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientNetv2 = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/classification/2\"\n",
    "pre_trained_model = hub.KerasLayer(efficientNetv2, input_shape=(224, 224, 3), trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a4b2e",
   "metadata": {},
   "source": [
    "### Fonction objectif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a438cd",
   "metadata": {},
   "source": [
    "Vous devez définir une fonction objectif que vous souhaitez optimiser. Cette fonction prendra les hyperparamètres comme arguments et renverra une valeur que vous souhaitez minimiser ou maximiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    # Paramètres à optimiser\n",
    "    \n",
    "     # Nombre de couches cachées\n",
    "    n_hidden = trial.suggest_int('n_hidden', 1, 5)\n",
    "    n_units = trial.suggest_int('n_units', 32, 128)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    dropout_rates = [trial.suggest_float(f'dropout_layer_{i}', 0.0, 0.5) for i in range(num_hidden_layers)]\n",
    "\n",
    "\n",
    "    # Créer le modèle EfficientNetv2 pré-entraîné avec des couches gelées (doit être importé aupravant)\n",
    "    base_model = pre_trained_model\n",
    "\n",
    "\n",
    "    # Ajouter des couches personnalisées pour la classification\n",
    "    x = base_model.output\n",
    "    \n",
    "    for i in range(n_hidden):\n",
    "        # Ajouter une couche dense avec le nombre d'unités spécifié\n",
    "        x = tf.keras.layers.Dense(n_units, activation='relu')(x)\n",
    "                # Ajouter une couche de dropout avec le taux spécifié\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i])(x)\n",
    "\n",
    "\n",
    "    # Sortie du modèle\n",
    "    predictions = Dense(10, activation='softmax')(x)\n",
    "    model = Model(inputs=pre_trained_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "    # Compilation du modèle\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1985405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    optuna_history = model.fit(ds_train, \n",
    "                               validation_data = ds_val,\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=20,\n",
    "                               callbacks = [tensorboard, early_stopping, reduceLR, checkpoint, time_callback, TON]\n",
    "                               verbose=0)\n",
    "    \n",
    "    score = model.evaluate(ds_test)[1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f33e5",
   "metadata": {},
   "source": [
    "### Etude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a43e194",
   "metadata": {},
   "source": [
    "Vous devez créer un objet d'étude Optuna et spécifier la direction de l'optimisation (minimisation ou maximisation) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a91797",
   "metadata": {},
   "source": [
    "La différence entre les modes \"minimize\" et \"maximize\" dans une étude Optuna réside dans la manière dont Optuna interprète la fonction objectif que vous cherchez à optimiser.\n",
    "\n",
    "**Minimize (Minimiser) :**\n",
    "\n",
    "Lorsque vous spécifiez direction='minimize' lors de la création de votre étude Optuna, vous indiquez à Optuna que vous cherchez à minimiser la valeur de la fonction objectif.\n",
    "Cela signifie que vous cherchez à obtenir la plus petite valeur possible de la fonction objectif. Par exemple, si vous utilisez la perte d'un modèle de machine learning comme fonction objectif, vous souhaitez minimiser cette perte (c'est-à-dire obtenir une perte aussi faible que possible).\n",
    "\n",
    "\n",
    "**Maximize (Maximiser) :**\n",
    "\n",
    "En revanche, lorsque vous spécifiez direction='maximize', vous indiquez à Optuna que vous cherchez à maximiser la valeur de la fonction objectif.\n",
    "Cela signifie que vous cherchez à obtenir la plus grande valeur possible de la fonction objectif. Par exemple, si vous cherchez à maximiser la précision d'un modèle de classification, vous souhaitez obtenir une précision aussi élevée que possible.\n",
    "Le choix entre \"minimize\" et \"maximize\" dépend du problème que vous résolvez et de la manière dont vous définissez votre fonction objectif. Par exemple, si vous cherchez à minimiser les erreurs, les pertes ou les coûts, vous utiliserez généralement \"minimize\". Si vous cherchez à maximiser les performances, les scores ou les gains, vous utiliserez généralement \"maximize\".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Voici un exemple concret :** supposons que vous entraîniez un modèle de classification et que votre fonction objectif est la précision du modèle. Dans ce cas, vous voudriez spécifier direction='maximize' car vous cherchez à obtenir la meilleure précision possible. D'un autre côté, si vous optimisez la perte du modèle, vous spécifierez direction='minimize' car vous voulez minimiser la perte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cdaa79",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da18474",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7fb00e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "log_dir = '../tensor_board_logs'\n",
    "tensorboard = callbacks.TensorBoard(log_dir = log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d7833",
   "metadata": {},
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e11b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor = 'val_accuracy', \n",
    "                               min_delta = 0.03,\n",
    "                               patience = 8,\n",
    "                               verbose = 1,\n",
    "                               mode = 'auto',\n",
    "                               restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a7771",
   "metadata": {},
   "source": [
    "### Reduce LearningRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14807e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceLR = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                             min_delta = 0.01,\n",
    "                             patience = 5,\n",
    "                             factor = 0.15, \n",
    "                             cooldown = 3,\n",
    "                             verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e83245",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "951ce051",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='../model/checkpoint_model', monitor='val_accuracy', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2501917",
   "metadata": {},
   "source": [
    "### Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2cac6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimingCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.starttime = timer()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        endtime = timer()\n",
    "        elapsed_time = endtime - self.starttime\n",
    "        self.logs.append(elapsed_time)\n",
    "        print(f\"Epoch {epoch + 1} took {elapsed_time:.2f} seconds\")\n",
    "\n",
    "time_callback = TimingCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b84662a",
   "metadata": {},
   "source": [
    "### Terminate on NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d566542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TON = TerminateOnNaN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2574357",
   "metadata": {},
   "source": [
    "# Pipeline Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf83bad7",
   "metadata": {},
   "source": [
    "### Import des DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a7114e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images chargées pour df: 64372\n",
      "Nb especes dans df: 10\n",
      "Nombre d'images chargées pour df_ech: 6437\n",
      "Nb especes dans df_ech: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\AppData\\Local\\Temp\\ipykernel_1236\\465607992.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['image_url'] = df['image_url'].str.replace('.../images/', chemin_images)\n"
     ]
    }
   ],
   "source": [
    "# Rappel : Utiliser df_ech pour les tests (entrainement rapide du modèle mais accuracy faible)\n",
    "df, df_ech = import_df(chemin_images, chemin_csv, pourcentage_echantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1997289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préciser sur quelles données travailler (df_ech est un echantillon permettant de réduire le temps d'entrainement pour effectuer des tests)\n",
    "# Commenter/Decommenter la ligne souhaitée\n",
    "#donnees_training = df_ech\n",
    "donnees_training = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc53b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facteurs d'oversampling des classes : \n",
      " Agaricales : x1 \n",
      " Agaricus: x2.5 \n",
      " Amanita : x2 \n",
      " Cortinarius: x1.5 \n",
      " Entoloma : x2.5 \n",
      " Inocybe: x2 \n",
      " Mycena : x2 \n",
      " Popyporales: x1.8 \n",
      " Psathyrella : x2 \n",
      " Russula: x1.5 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_lien</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Agaricales</th>\n",
       "      <td>11517</td>\n",
       "      <td>11517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agaricus</th>\n",
       "      <td>4692</td>\n",
       "      <td>4692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amanita</th>\n",
       "      <td>4987</td>\n",
       "      <td>4987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cortinarius</th>\n",
       "      <td>7352</td>\n",
       "      <td>7352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entoloma</th>\n",
       "      <td>4209</td>\n",
       "      <td>4209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inocybe</th>\n",
       "      <td>5607</td>\n",
       "      <td>5607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mycena</th>\n",
       "      <td>5342</td>\n",
       "      <td>5342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polyporales</th>\n",
       "      <td>6864</td>\n",
       "      <td>6864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Psathyrella</th>\n",
       "      <td>5564</td>\n",
       "      <td>5564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russula</th>\n",
       "      <td>8238</td>\n",
       "      <td>8238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_lien  image_url\n",
       "label                             \n",
       "Agaricales        11517      11517\n",
       "Agaricus           4692       4692\n",
       "Amanita            4987       4987\n",
       "Cortinarius        7352       7352\n",
       "Entoloma           4209       4209\n",
       "Inocybe            5607       5607\n",
       "Mycena             5342       5342\n",
       "Polyporales        6864       6864\n",
       "Psathyrella        5564       5564\n",
       "Russula            8238       8238"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Représentation des classes\n",
    "print('Facteurs d\\'oversampling des classes : \\n',\n",
    "      'Agaricales : x1 \\n',\n",
    "      'Agaricus: x2.5 \\n',\n",
    "      'Amanita : x2 \\n',\n",
    "      'Cortinarius: x1.5 \\n',\n",
    "      'Entoloma : x2.5 \\n',\n",
    "      'Inocybe: x2 \\n',\n",
    "      'Mycena : x2 \\n',\n",
    "      'Popyporales: x1.8 \\n',\n",
    "      'Psathyrella : x2 \\n',\n",
    "      'Russula: x1.5 \\n')\n",
    "\n",
    "donnees_training.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controle de la présence des fichiers images\n",
    "controle_presence_fichiers(donnees_training, chemin_images)\n",
    "\n",
    "# On supprime ensuite la colonne image_lien qui ne sert qu'à controler la présence des fichiers.\n",
    "donnees_training.drop('image_lien', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f9720ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les classes à oversampler :\n",
    "oversample_cls = ['Agaricus', 'Amanita', 'Cortinarius', 'Entoloma', 'Inocybe', 'Mycena', 'Polyporales', 'Psathyrella', 'Russula']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468bd3d8",
   "metadata": {},
   "source": [
    "### Construction des jeux de données (train, test et validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c73f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = donnees_training.drop('label', axis=1)\n",
    "target = donnees_training['label']\n",
    "\n",
    "s = LabelEncoder()\n",
    "target = s.fit_transform(target) # Encodage de la variable 'label'\n",
    "\n",
    "# On construit le jeu d'entrainnement. X_temp et y_temps servent pour la construction des jeux de test et validation\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data, target, test_size=0.25, random_state=10)\n",
    "\n",
    "# On split les temp en 50% pour test, 50% pour validation\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a765e8b",
   "metadata": {},
   "source": [
    "### Construction des dataset Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7c514280",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# Les datasets sont créés à partir de la fonction create_tf_dataset définie dans la partie 'Fonctions'\n",
    "ds_train= create_tf_dataset(X_train.image_url, y_train, batch_size)\n",
    "ds_test = create_tf_dataset(X_test.image_url, y_test, batch_size)\n",
    "ds_val = create_tf_dataset(X_val.image_url, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ad686",
   "metadata": {},
   "source": [
    "# Optuna - Otimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f11a8",
   "metadata": {},
   "source": [
    "Utilisez la méthode optimize de l'objet d'étude en spécifiant la fonction objectif et le nombre d'essais que vous souhaitez effectuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /!\\ Attention l'entrainement peut être très très long\n",
    "study.optimize(objective, n_trials=15, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceb12b2",
   "metadata": {},
   "source": [
    "### Résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e188a39",
   "metadata": {},
   "source": [
    "Une fois l'optimisation terminée, vous pouvez accéder aux meilleurs hyperparamètres et à la meilleure valeur obtenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98714eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "best_value = study.best_value\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80452a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_viz.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08066821",
   "metadata": {},
   "source": [
    "# Optuna - Construction best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412cfa8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Construire avec les meilleurs paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d40a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler le modèle\n",
    "best_model = create_model(study.best_trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fadb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9ef46",
   "metadata": {},
   "source": [
    "# Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8506db40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 34s 3s/step - loss: 1.7338 - accuracy: 0.4099\n",
      "Test accuracy: 0.40993788838386536\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = best_model.evaluate(ds_test)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e597c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafbc0e2",
   "metadata": {},
   "source": [
    "# Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a0aac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changer le nom du modèle si il s'agit d'un nouvel entrainement\n",
    "\n",
    "# Save en dur\n",
    "#nom_modele = '../model/gpot_v01_optuna_echantillon'\n",
    "\n",
    "# Save sur GDrive\n",
    "nom_modele =  '/content/drive/MyDrive/SAS/model/gpot_v01_optuna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(nom_modele)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
